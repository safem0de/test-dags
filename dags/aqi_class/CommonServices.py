import json
import os
import pandas as pd
import numpy as np

from airflow.providers.postgres.hooks.postgres import PostgresHook

class CommonServices:
    """Class ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£ Database, API ‡πÅ‡∏•‡∏∞‡∏Å‡∏≤‡∏£‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÑ‡∏ü‡∏•‡πå JSON"""

    def __init__(self):
        pass


    def map_region(self, state:str):
        """
        ‡∏Ñ‡∏∑‡∏ô‡∏Ñ‡πà‡∏≤‡∏†‡∏≤‡∏Ñ (region) ‡∏ï‡∏≤‡∏°‡∏ä‡∏∑‡πà‡∏≠‡∏à‡∏±‡∏á‡∏´‡∏ß‡∏±‡∏î
        ‡πÄ‡∏´‡∏ô‡∏∑‡∏≠	    North
        ‡∏≠‡∏µ‡∏™‡∏≤‡∏ô	    Northeast
        ‡∏Å‡∏•‡∏≤‡∏á	   Central
        ‡∏ï‡∏∞‡∏ß‡∏±‡∏ô‡∏≠‡∏≠‡∏Å	East
        ‡∏ï‡∏∞‡∏ß‡∏±‡∏ô‡∏ï‡∏Å     West
        ‡πÉ‡∏ï‡πâ         South
        """

        region_mapping = {
            "North": {'Chiang Mai', 'Chiang Rai', 'Lampang', 'Lamphun', 'Nan', 'Phayao', 'Phrae', 'Mae Hong Son', 'Sukhothai', 'Tak', 'Uttaradit'},
            "Northeast": {'Amnat Charoen', 'Buriram', 'Chaiyaphum', 'Changwat Bueng Kan', 'Changwat Ubon Ratchathani', 'Kalasin', 'Khon Kaen', 'Loei', 'Maha Sarakham', 'Mukdahan',
                    'Nakhon Phanom', 'Nakhon Ratchasima', 'Nong Bua Lamphu', 'Nong Khai', 'Roi Et', 'Sakon Nakhon', 
                    'Sisaket', 'Surin', 'Ubon Ratchathani', 'Udon Thani', 'Yasothon'},
            "Central": {'Ang Thong', 'Bangkok', 'Chai Nat', 'Kamphaeng Phet', 'Lopburi', 'Nakhon Nayok', 'Nakhon Pathom',
                    'Nakhon Sawan', 'Nonthaburi', 'Pathum Thani', 'Phetchabun', 'Phichit', 'Phitsanulok', 'Phra Nakhon Si Ayutthaya',
                    'Samut Prakan', 'Samut Sakhon', 'Samut Songkhram', 'Sara Buri', 'Sing Buri', 'Suphan Buri','Uthai Thani'},
            "East": {'Chachoengsao', 'Chanthaburi', 'Chon Buri', 'Prachin Buri', 'Rayong', 'Sa Kaeo', 'Trat'},
            "West": {'Kanchanaburi', 'Phetchaburi', 'Prachuap Khiri Khan', 'Ratchaburi', 'Tak'},
            "South": {'Chumphon', 'Krabi', 'Nakhon Si Thammarat', 'Narathiwat', 'Pattani', 'Phangnga', 'Phatthalung',
                    'Phuket', 'Ranong', 'Satun', 'Songkhla', 'Surat Thani', 'Trang', 'Yala'}
        }
        
        for region, states in region_mapping.items():
            if state in states:
                return region
        
        return "Unknown"  # Default ‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•
    

    def json_to_list(self, filename: str, parent_key: str, child_key: str) -> list:
        """
        Extracts a list of values from a JSON file based on the specified keys.

        :param filename: Path to the JSON file.
        :param parent_key: The key that contains the list of dictionaries.
        :param child_key: The key to extract values from each dictionary inside the parent_key list.
        :return: A list of extracted values.
        """
        try:
            # ‡∏≠‡πà‡∏≤‡∏ô JSON ‡∏à‡∏≤‡∏Å‡πÑ‡∏ü‡∏•‡πå
            with open(filename, "r", encoding="utf-8") as f:
                data = json.load(f)

            # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤ JSON ‡πÄ‡∏õ‡πá‡∏ô dictionary ‡πÅ‡∏•‡∏∞‡∏°‡∏µ parent_key ‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£
            if not isinstance(data, dict) or parent_key not in data:
                print(f"‚ùå Error: JSON file does not contain expected '{parent_key}' key.")
                return []

            # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤ parent_key ‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏õ‡πá‡∏ô list ‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà
            if not isinstance(data[parent_key], list):
                print(f"‚ùå Error: '{parent_key}' is not a list.")
                return []

            # ‡∏î‡∏∂‡∏á‡∏Ñ‡πà‡∏≤‡∏à‡∏≤‡∏Å child_key ‡πÉ‡∏ô‡πÅ‡∏ï‡πà‡∏•‡∏∞ dictionary
            return [item.get(child_key, None) for item in data[parent_key]]

        except FileNotFoundError:
            print(f"‚ùå Error: File '{filename}' not found.")
            return []

        except json.JSONDecodeError:
            print(f"‚ùå Error: File '{filename}' is not a valid JSON file.")
            return []

    # ‚úÖ ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö Connection ‡∏Ç‡∏≠‡∏á Database
    def check_conn_string(self, conn_id:str) -> bool:
        try:
            pg_hook = PostgresHook(postgres_conn_id=conn_id)
            connection = pg_hook.get_conn()
            cursor = connection.cursor()
            cursor.execute("SELECT 1;")
            connection.close()
            print(f"‚úÖ Connection {conn_id} is valid!")
            return True
        except Exception as e:
            print(f"‚ùå Connection failed: {str(e)}")
            return False
    

    def create_database(self, conn_id:str, database_name:str):
        self.check_conn_string(conn_id)
        pg_hook = PostgresHook(postgres_conn_id=conn_id)
        connection = pg_hook.get_conn()
        connection.set_isolation_level(0)
        cursor = connection.cursor()

        cursor.execute(f"SELECT 1 FROM pg_database WHERE datname = '{database_name}';")
        exists = cursor.fetchone()
        
        if not exists:
            cursor.execute(f"CREATE DATABASE {database_name};")
            connection.commit()
        
        connection.close()
        print(f"‚úÖ Database '{database_name}' is ready!")


    # ‚úÖ Execute SQL Query
    def execute_sql(self, conn_id:str, database_name:str, sql_statement:str, parameters: tuple = None) -> None:
        try:
            pg_hook = PostgresHook(postgres_conn_id=conn_id, database=database_name)
            connection = pg_hook.get_conn()
            cursor = connection.cursor()

            if parameters:
                cursor.execute(sql_statement, parameters)
            else:
                cursor.execute(sql_statement)

            connection.commit()
            print(f"‚úÖ SQL executed successfully on database: {database_name}")
        except Exception as e:
            connection.rollback()
            print(f"‚ùå Error executing SQL on {database_name}: {str(e)}")
        finally:
            cursor.close()
            connection.close()


    # ‚úÖ ‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÑ‡∏ü‡∏•‡πå JSON ‡∏ñ‡πâ‡∏≤‡∏¢‡∏±‡∏á‡πÑ‡∏°‡πà‡∏°‡∏µ
    def create_file_if_not_exist(self, file_path: str, data: dict) -> None:
        
        if not isinstance(data, dict):  # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤ data ‡πÄ‡∏õ‡πá‡∏ô dict
            print("‚ùå Error: Provided data is not a valid JSON dictionary.")
            return

        if not os.path.exists(file_path):
            with open(file_path, 'w') as file:
                json.dump(data, file, indent=4)
            print(f"‚úÖ File created: {file_path}")
        else:
            print(f"‚ö†Ô∏è File already exists: {file_path}")


    def check_file_exists(self, file_path) -> bool:
        if not os.path.exists(file_path):
            return False
        return True
    

    def check_data_quality(self, df:pd.DataFrame, expected_types:dict = None) -> pd.DataFrame:
        """
        ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏Ñ‡∏∏‡∏ì‡∏†‡∏≤‡∏û‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÉ‡∏ô DataFrame
        """
        print("üîç ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏Ñ‡∏∏‡∏ì‡∏†‡∏≤‡∏û‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• DataFrame...\n")
        
        # ‚úÖ ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö Missing Values (‡∏Ñ‡πà‡∏≤ NaN / NULL)
        missing_values = df.isnull().sum()
        missing_percentage = (df.isnull().sum() / len(df)) * 100

        # ‚úÖ ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏Ñ‡πà‡∏≤‡∏ã‡πâ‡∏≥‡∏ã‡πâ‡∏≠‡∏ô (Duplicated Rows)
        duplicate_count = pd.Series(df.duplicated().sum(), index=["Duplicate Count"])

        type_mismatch_series = pd.Series(dtype=int)
        if expected_types:
            type_mismatch = {
                col: df[col].apply(lambda x: not isinstance(x, expected_types.get(col, type(x)))).sum()
                for col in df.columns if col in expected_types
            }
        
            type_mismatch_series = pd.Series(type_mismatch).fillna(0).astype(int)

        # ‚úÖ ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏Ñ‡πà‡∏≤‡∏ó‡∏µ‡πà‡∏≠‡∏¢‡∏π‡πà‡∏ô‡∏≠‡∏Å‡∏ä‡πà‡∏ß‡∏á (Outliers) ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Ñ‡πà‡∏≤‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏Ç
        numerical_columns = df.select_dtypes(include=[np.number]).columns
        outliers = {}
        for col in numerical_columns:
            Q1 = df[col].quantile(0.25)
            Q3 = df[col].quantile(0.75)
            IQR = Q3 - Q1
            lower_bound = Q1 - 1.5 * IQR
            upper_bound = Q3 + 1.5 * IQR
            outliers[col] = df[(df[col] < lower_bound) | (df[col] > upper_bound)][col].count()

        outliers_series = pd.Series(outliers).fillna(0).astype(int)

        # ‚úÖ ‡∏£‡∏ß‡∏°‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î‡πÄ‡∏õ‡πá‡∏ô DataFrame
        quality_report = pd.DataFrame({
            "Missing Values": missing_values,
            "Missing %": missing_percentage,
            "Type Mismatch": type_mismatch_series,
            "Outliers": outliers_series
        }).fillna(0).astype(int)

        # ‚úÖ ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• Duplicates ‡πÄ‡∏õ‡πá‡∏ô‡πÅ‡∏ñ‡∏ß‡πÉ‡∏´‡∏°‡πà
        quality_report = pd.concat([quality_report, duplicate_count.to_frame().T])
        print("‚úÖ ‡∏Å‡∏≤‡∏£‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡πÄ‡∏™‡∏£‡πá‡∏à‡∏™‡∏¥‡πâ‡∏ô!")
        return quality_report
        